{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install cohere sentence_transformers torch datasets openai"
      ],
      "metadata": {
        "id": "il4r__K3E6j4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import gc\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import cohere\n",
        "from openai import OpenAI\n",
        "import time\n",
        "\n",
        "import psutil"
      ],
      "metadata": {
        "id": "E9ZTWPeTa9uL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dictionary to keep track of performance metrics\n",
        "performance_dict = {}"
      ],
      "metadata": {
        "id": "zdejLIKUNq0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to retrieve Nebula Embeddings\n",
        "def nebula_embedder(sent):\n",
        "    url = \"https://api-nebula.symbl.ai/v1/model/embed\"\n",
        "\n",
        "    payload = json.dumps({\n",
        "    \"text\": f\"\"\"{sent}\"\"\"\n",
        "    })\n",
        "    headers = {\n",
        "    'ApiKey': 'nebula_api',\n",
        "    'Content-Type': 'application/json'\n",
        "    }\n",
        "\n",
        "    response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
        "\n",
        "    response_json = response.json()\n",
        "\n",
        "    return response_json['embedding']\n",
        "\n",
        "#Function to retrieve Cohere Embeddings\n",
        "def cohere_embedder(sent, model='embed-english-v3.0', input_type='classification'):\n",
        "    co = cohere.Client('<cohere_api_key>')\n",
        "\n",
        "    response = co.embed(\n",
        "    texts=[sent],\n",
        "    model=model,\n",
        "    input_type=input_type\n",
        "    )\n",
        "\n",
        "    return(response.embeddings[0])\n",
        "\n",
        "#Function to retrieve Open API Embeddings\n",
        "def openai_embedder(sent, model=\"text-embedding-3-small\"):\n",
        "    client = OpenAI(api_key='<openai_api_key>')\n",
        "    sent = sent.replace(\"\\n\", \" \")\n",
        "    return client.embeddings.create(input = [sent], model=model).data[0].embedding"
      ],
      "metadata": {
        "id": "SGwgVgqaFXSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset 1: Polarity Detection in Amazon Reviews"
      ],
      "metadata": {
        "id": "6bj9vgDpr7aW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "amazon_polarity_dataset = load_dataset(\"amazon_polarity\")\n",
        "amazon_polarity_subset = amazon_polarity_dataset[\"train\"].select(range(1000))"
      ],
      "metadata": {
        "id": "potz0YYlFYMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def create_dataloaders(encoded_data,batch_size):\n",
        "\n",
        "    # Split the dataset into training and validation sets\n",
        "    train_data, val_data = train_test_split(encoded_data, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Function to collate batch\n",
        "    def collate_batch(batch):\n",
        "        labels, embeddings = zip(*batch)\n",
        "        labels = torch.tensor(labels, dtype=torch.long)\n",
        "        embeddings = torch.stack(embeddings, dim=0).to(device)\n",
        "        return labels, embeddings\n",
        "\n",
        "    # Prepare data loaders\n",
        "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
        "    val_loader = DataLoader(val_data, batch_size=batch_size, collate_fn=collate_batch)\n",
        "\n",
        "    # Example usage:\n",
        "    for labels, embeddings in train_loader:\n",
        "        print(f\"Dataloader Sample Shape: {labels.shape}, {embeddings.shape}\")\n",
        "        break\n",
        "    return train_loader, val_loader"
      ],
      "metadata": {
        "id": "5etD1mOoQg81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Class Definition\n",
        "class TextClassifier(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, output_dim):\n",
        "        super(TextClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, text):\n",
        "        out = torch.relu(self.fc1(text))\n",
        "        out = self.fc2(out)\n",
        "        out = self.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "#Create a function for Training loop\n",
        "def train(model, train_dataloader, epoch_num, optimizer, criterion, model_path):\n",
        "    \"\"\"\n",
        "    Performs training of the given model on the training data with the associated parameters\n",
        "\n",
        "    Parameters:\n",
        "        Model instance to be trained -> torch.nn.Module\n",
        "        Training data loader -> torch.utils.data.DataLoader\n",
        "        Number of training epochs -> int\n",
        "        Optimizer to be used while training -> torch.optim\n",
        "        Loss function that is to be optimized -> torch.nn.modules.loss\n",
        "        Name to be used while saving the model -> str\n",
        "\n",
        "    Returns:\n",
        "        Training Loss at after each epoch -> list[float]\n",
        "    \"\"\"\n",
        "\n",
        "    train_loss = []\n",
        "    model.train()\n",
        "    print(f\"\\nTraining..\")\n",
        "    for epoch in range(epoch_num):\n",
        "        epoch_loss = 0\n",
        "        for _, batch in enumerate(train_dataloader):\n",
        "            labels, texts = batch\n",
        "            optimizer.zero_grad()\n",
        "            #sentence_embeddings = model.encode(texts)\n",
        "            predictions = model(texts)\n",
        "            loss = criterion(predictions, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        train_loss.append(epoch_loss)\n",
        "        print(f\"Epoch {epoch+1} Loss: {epoch_loss}\")\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "\n",
        "    return train_loss\n",
        "\n",
        "#Create a function for Evaluation loop\n",
        "def evaluate(model, test_dataloader):\n",
        "    \"\"\"\n",
        "    Performs evaluation of the given model using classification accuracy on the test data with the associated parameters\n",
        "\n",
        "    Parameters:\n",
        "        Model instance to be trained -> torch.nn.Module\n",
        "        Test data loader -> torch.utils.data.DataLoader\n",
        "\n",
        "    Returns:\n",
        "        Classification Accuracy on the Test Set -> float\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for _, batch in enumerate(test_dataloader):\n",
        "            labels, texts = batch\n",
        "            #sentence_embeddings = model.encode(texts)\n",
        "            predictions = model(texts)\n",
        "            preds = predictions.argmax(1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    acc_score = accuracy_score(all_labels, all_preds)\n",
        "    print(f\"\\nTest Accuracy: {acc_score}\")\n",
        "\n",
        "    return acc_score"
      ],
      "metadata": {
        "id": "0V5wf487f8M9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to train and evaluate the model for the chosen embeddings\n",
        "def train_and_evaluate_d1(embedder, model_path, dataset=amazon_polarity_subset):\n",
        "    \"\"\"\n",
        "    Trains and evaluates a binary sentiment analyzer using the given embedder to encode the inputs\n",
        "\n",
        "    Parameters:\n",
        "    The embedding model of choice -> func\n",
        "    Local path where the trained model's parameters will be saved -> str\n",
        "    Dataset containing amazon reviews mapped to their true sentiment -> datasets.dataset\n",
        "\n",
        "    Returns:\n",
        "    Dictionary contatinig the evaluation metrics including training loss and test accuracy, and the inference latency -> Dict[str->[int]]\n",
        "    \"\"\"\n",
        "\n",
        "    #Define model parameters\n",
        "    BATCH_SIZE = 32\n",
        "    HIDDEN_DIM = 64\n",
        "    OUTPUT_DIM = 2  # Number of classes\n",
        "    NUM_EPOCHS = 10\n",
        "    LEARNING_RATE = 0.001\n",
        "\n",
        "    memory_usage_init = psutil.Process().memory_info().rss\n",
        "\n",
        "    #Preprocess and encode dataset using Nebula Embeddings\n",
        "    start = time.time()\n",
        "    encoded_data = [(example[\"label\"], torch.tensor(embedder(example[\"content\"]))) for example in dataset]\n",
        "    end = time.time()\n",
        "    inference_time = round(end-start,2)\n",
        "\n",
        "    train_loader, test_loader = create_dataloaders(encoded_data, BATCH_SIZE)\n",
        "\n",
        "    #Set the embedding dimension\n",
        "    EMBEDDING_DIM = len(encoded_data[0][1])\n",
        "\n",
        "    #Initialize model, criterion, optimizer\n",
        "    classifier_model = TextClassifier(EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(classifier_model.parameters(), lr=LEARNING_RATE)\n",
        "    classifier_model.to(device)\n",
        "\n",
        "    #Perform training\n",
        "    train_loss = train(model=classifier_model, train_dataloader=train_loader, epoch_num=NUM_EPOCHS, optimizer=optimizer,\n",
        "                       criterion=criterion, model_path=model_path)\n",
        "\n",
        "    #Perform evaluation\n",
        "    classifier_model = TextClassifier(EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
        "    classifier_model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "    eval_acc = evaluate(classifier_model, test_loader)\n",
        "\n",
        "    memory_usage_final = psutil.Process().memory_info().rss\n",
        "\n",
        "    memory_usage = (memory_usage_final - memory_usage_init)/(1024 ** 2)\n",
        "\n",
        "    classifier_model.cpu()\n",
        "    del classifier_model\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return {'eval_metrics':[train_loss,eval_acc],'compute_metrics':[inference_time, memory_usage]}"
      ],
      "metadata": {
        "id": "MflHkBLi0U4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Nebula Embeddings Model"
      ],
      "metadata": {
        "id": "D9JoFGxMsUoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Embedding Model: Nebula\\n\")\n",
        "\n",
        "metrics = train_and_evaluate_d1(embedder=nebula_embedder, model_path='nebula_dataset1.pth')\n",
        "performance_dict['nebula_d1'] = metrics"
      ],
      "metadata": {
        "id": "KefnbPC0hjlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Open AI Embeddings Model"
      ],
      "metadata": {
        "id": "WdxetEm7s5QA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Embedding Model: Open AI\\n\")\n",
        "\n",
        "metrics = train_and_evaluate_d1(embedder=openai_embedder, model_path='openai_dataset1.pth')\n",
        "performance_dict['open_ai_d1'] = metrics"
      ],
      "metadata": {
        "id": "kJcHN8S60Ioo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Cohere Embeddings Model"
      ],
      "metadata": {
        "id": "eWgV69t8s9Lt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Embedding Model: Cohere\\n\")\n",
        "\n",
        "metrics = train_and_evaluate_d1(embedder=cohere_embedder, model_path='cohere_dataset1.pth')\n",
        "performance_dict['cohere_d1'] = metrics"
      ],
      "metadata": {
        "id": "on3Jx-ESE_GZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset 2: Sentiment detection in Banking Data"
      ],
      "metadata": {
        "id": "sesmbZcjswlv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "banking_dataset = load_dataset(\"banking77\")\n",
        "banking_subset = banking_dataset[\"train\"].select(range(1000))"
      ],
      "metadata": {
        "id": "5KR1Z2ZDFeJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to train and evaluate the model for the chosen embeddings\n",
        "def train_and_evaluate_d2(embedder, model_path, dataset=banking_subset):\n",
        "    \"\"\"\n",
        "    Trains and evaluates a multi-class classifier using the given embedder to encode the inputs\n",
        "\n",
        "    Parameters:\n",
        "    The embedding model of choice -> func\n",
        "    Local path where the trained model's parameters will be saved -> str\n",
        "    Dataset containing customer queries mapped to their true intent -> datasets.dataset\n",
        "\n",
        "    Returns:\n",
        "    Dictionary contatinig the evaluation metrics including training loss and test accuracy, and the inference latency -> Dict[str->[int]]\n",
        "    \"\"\"\n",
        "\n",
        "    #Define model parameters\n",
        "    BATCH_SIZE = 32\n",
        "    HIDDEN_DIM = 64\n",
        "    OUTPUT_DIM = 77  # Number of classes\n",
        "    NUM_EPOCHS = 10\n",
        "    LEARNING_RATE = 0.001\n",
        "\n",
        "    memory_usage_init = psutil.virtual_memory().percent\n",
        "\n",
        "    #Preprocess and encode dataset using Nebula Embeddings\n",
        "    start = time.time()\n",
        "    encoded_data = [(example[\"label\"], torch.tensor(embedder(example[\"text\"]))) for example in dataset]\n",
        "    end = time.time()\n",
        "    inference_time = round(end-start,2)\n",
        "\n",
        "    train_loader, test_loader = create_dataloaders(encoded_data, BATCH_SIZE)\n",
        "\n",
        "    #Set the embedding dimension\n",
        "    EMBEDDING_DIM = len(encoded_data[0][1])\n",
        "\n",
        "    #Initialize model, criterion, optimizer\n",
        "    classifier_model = TextClassifier(EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(classifier_model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    #Perform training\n",
        "    train_loss = train(model=classifier_model, train_dataloader=train_loader, epoch_num=NUM_EPOCHS, optimizer=optimizer,\n",
        "                       criterion=criterion, model_path=model_path)\n",
        "\n",
        "    #Perform evaluation\n",
        "    classifier_model = TextClassifier(EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
        "    classifier_model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "    eval_acc = evaluate(classifier_model, test_loader)\n",
        "\n",
        "    memory_usage_final = psutil.virtual_memory().percent\n",
        "\n",
        "    memory_usage = memory_usage_final - memory_usage_init\n",
        "\n",
        "    classifier_model.cpu()\n",
        "    del classifier_model\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return {'eval_metrics':[train_loss,eval_acc],'compute_metrics':inference_time}"
      ],
      "metadata": {
        "id": "j0FmmS9RFVyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Nebula Embeddings Model"
      ],
      "metadata": {
        "id": "ofH-1FG4FVyd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Embedding Model: Nebula\\n\")\n",
        "\n",
        "metrics = train_and_evaluate_d2(embedder=nebula_embedder, model_path='nebula_dataset2.pth')\n",
        "performance_dict['nebula_d2'] = metrics"
      ],
      "metadata": {
        "id": "KLRkKs9EFVye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Open AI Embeddings Model"
      ],
      "metadata": {
        "id": "-_SLLACQFVye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Embedding Model: Open AI\\n\")\n",
        "\n",
        "metrics = train_and_evaluate_d2(embedder=openai_embedder, model_path='openai_dataset2.pth')\n",
        "performance_dict['open_ai_d2'] = metrics"
      ],
      "metadata": {
        "id": "oYhJvXhcFVyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Cohere Embeddings Model"
      ],
      "metadata": {
        "id": "a0XUtsNeFVyf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Embedding Model: Cohere\\n\")\n",
        "\n",
        "metrics = train_and_evaluate_d2(embedder=cohere_embedder, model_path='cohere_dataset2.pth')\n",
        "performance_dict['cohere_d2'] = metrics"
      ],
      "metadata": {
        "id": "a8HdcPGlFVyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset 3: Sentences Involving Compositional Knowldedge (SICK)"
      ],
      "metadata": {
        "id": "0ueu4hxDF6bY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sick_dataset = load_dataset(\"sick\")\n",
        "test_set = sick_dataset['test'].select(range(1000))"
      ],
      "metadata": {
        "id": "XPSRlgszGEyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_d3(embedder,data=test_set):\n",
        "    \"\"\"\n",
        "    Computes the Pearson's corerlation coefficient between the true and predicted similarity scores for the given dataset\n",
        "\n",
        "    Parameters:\n",
        "    The embedding model of choice -> func\n",
        "    Dataset containing sentence pairs and true relatedness score for each pair -> datasets.dataset\n",
        "\n",
        "    Returns:\n",
        "    Dictionary contatinig the Pearson's correlation coefficient and the inference latency -> Dict[str->[int]]\n",
        "    \"\"\"\n",
        "\n",
        "    #Initialize the arrays to store the true and predicted similarity scores\n",
        "    true_similarity_scores = []\n",
        "    pred_similarity_scores = []\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    #Counter variable to keep track of number of records processed, and invoke sleep once 100 records are done\n",
        "    counter = 0\n",
        "\n",
        "    for iter in test_set:\n",
        "\n",
        "        counter+=1\n",
        "\n",
        "        sent1 = iter['sentence_A']\n",
        "        sent2 = iter['sentence_B']\n",
        "        #Extract the true relatedness score for the given pair of sentences\n",
        "        relatedness_score = iter['relatedness_score']\n",
        "        true_similarity_scores.append(relatedness_score)\n",
        "\n",
        "        #Encode the pair of sentences using the chosen embedder and compute cosine similarity score\n",
        "        encoded_sent1 = embedder(sent1)\n",
        "        encoded_sent2 = embedder(sent2)\n",
        "        cosine_score = cosine_similarity([encoded_sent1],[encoded_sent2])[0][0]\n",
        "        pred_similarity_scores.append(cosine_score)\n",
        "\n",
        "    end = time.time()\n",
        "\n",
        "    inference_time = end - start\n",
        "\n",
        "    pearson_corr, _ = pearsonr(true_similarity_scores, pred_similarity_scores)\n",
        "\n",
        "    print(f\"Pearson's Correaltion: {pearson_corr}\")\n",
        "\n",
        "    return {'eval_metrics':[pearson_corr],'compute_metrics':[inference_time]}"
      ],
      "metadata": {
        "id": "ALapU4yVh2cp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Nebula Embeddings Model"
      ],
      "metadata": {
        "id": "z76ujnh1l4PU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Embedding Model: Nebula\\n\")\n",
        "\n",
        "metrics = evaluate_d3(embedder=nebula_embedder)\n",
        "performance_dict['nebula_d3'] = metrics"
      ],
      "metadata": {
        "id": "UR1bQg_5l4PU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "performance_dict['nebula_d3']"
      ],
      "metadata": {
        "id": "XE8mxc1Ip2wE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Open AI Embeddings Model"
      ],
      "metadata": {
        "id": "Rz5c5_HEl4PV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Embedding Model: Open AI\\n\")\n",
        "\n",
        "metrics = evaluate_d3(embedder=openai_embedder)\n",
        "performance_dict['open_ai_d3'] = metrics"
      ],
      "metadata": {
        "id": "ck3BN8N3l4PV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Cohere Embeddings Model"
      ],
      "metadata": {
        "id": "cGRY-Kkol4PW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Embedding Model: Cohere\\n\")\n",
        "\n",
        "metrics = evaluate_d3(embedder=cohere_embedder)\n",
        "performance_dict['cohere_d3'] = metrics"
      ],
      "metadata": {
        "id": "fWBuHxW9l4PW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "performance_dict"
      ],
      "metadata": {
        "id": "NgBUzgXY8Itb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
